---
title: $K$NN
author: "Sammy Twenhafel"
date: "02/10/2025"

format: 
  html:  # You will quite likely want to change all but the last one, to taste
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true
---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/knn.qmd) hosted on GitHub pages.

# 0. Quarto Type-setting

-   This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.
-   If you wish to use a similar header, here's is the format specification for this document:

``` email
format: 
  html:
    embed-resources: true
```

# 1. Setup

```{r}
library(tidyverse)
library(caret)
library(EnvStats)
library(fastDummies)
library(class)
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

## 2. $K$NN Concepts

> [TODO]{style="color:red;font-weight:bold"}: *Explain how the choice of K affects the quality of your prediction when using a* $K$ Nearest Neighbors algorithm.
>
> When a small number is chosen for the K, it makes it can make the model incredibly simple and may not take into account some nuiances of the model. As we saw in class a smaller K can lead to over fitting the data and lean too heavily on one of the features apart of the KNN model. As the K gets larger, more and more features may be considered when making the prediction. A larger value can normalize the model because it is looking at more points before making a prediction.

## 3. Feature Engineering

1.  Remove the taster_name column from the data.
2.  Create a version of the year column that is a *factor* (instead of numeric).
3.  Create dummy variables that indicate the presence of "cherry", "chocolate" and "earth" in the description.

-   Take care to handle upper and lower case characters.

4.  Create 3 new features that represent the interaction between time and the cherry, chocolate and earth inidicators.
5.  Remove the description column from the data.

```{r}
wino = wine%>%
  mutate(year_f = as.factor(year))%>%
  rename_all(funs(tolower(.))) %>% 
  rename_all(funs(str_replace_all(., "-", "_"))) %>% 
  rename_all(funs(str_replace_all(., " ", "_")))%>%
  mutate(note_cherry = str_detect(description,"cherry")) %>% 
  mutate(note_chocolate = str_detect(description,"chocolate")) %>%
  mutate(note_earth = str_detect(description,"earth"))%>%
  mutate(year_cherry = year*note_cherry)%>%
  mutate(year_choco = year*note_chocolate)%>%
  mutate(year_earth = year*note_earth)%>%
  select(-description)%>%
  select(-id)
  

```

## 4. Preprocessing

1.  Preprocess the dataframe from the previous code block using BoxCox, centering and scaling of the numeric features
2.  Create dummy variables for the `year` factor column

```{r}
#Preprocessing the dataframe
wino <- wino %>% 
  preProcess(method = c("BoxCox","center","scale")) %>% 
  predict(wino)


#Creating the dummy variable for year
wino = wino%>%
  dummy_cols(
    select_columns = "year_f",
    remove_most_frequent_dummy = T, 
    remove_selected_columns = T)
```

## 5. Running $K$NN

1.  Split the dataframe into an 80/20 training and test set
2.  Use Caret to run a $K$NN model that uses your engineered features to predict province

-   use 5-fold cross validated subsampling
-   allow Caret to try 15 different values for K

3.  Display the confusion matrix on the test data

```{r}
set.seed(505)
wine_index <- createDataPartition(wino$province, p = 0.8, list = FALSE)
train <- wino[ wine_index, ]
test <- wino[-wine_index, ]

#control <- rfeControl(functions = rfFuncs, method = "cv", number = 5)

#fit <- knn(
  #train = select(train,-province), 
  #test = select(test,-province), 
 # k=15, 
 # cl = train$province)

fit <- train(province ~ .,
             data = train, 
             method = "knn",
             tuneLength = 15,
             trControl = trainControl( method = "cv", number = 5))

confusionMatrix(predict(fit, test),factor(test$province))


```

## 6. Kappa

How do we determine whether a Kappa value is represents a good or bad outcome?

> [**TODO**]{style="color:red;font-weight:bold"}: *We can determine whether or not the Kappa value represents a good or bad outcome based on where it is between 0 and 1.0. In this case the Kappa was 0.367. This means that our K nearest neighbor model was just okay at predicting which province the wine came from.*

## 7. Improvement

How can we interpret the confusion matrix, and how can we improve in our predictions?

> [TODO]{style="color:red;font-weight:bold"}: *The confusion matrix tells us that the model was good at predicting California, but when it was predicting Oregon, it got it confused for California a lot of the time. We can improve our model by adding more features. This would allow for the model to see even more distinctions between the different "provinces."* You could also change the amount of K nearest neighbors the model tries while predicting.
